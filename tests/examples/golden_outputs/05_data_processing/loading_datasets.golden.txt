
==================================================
  Loading and Processing Datasets
==================================================

Part 1: Basic Data Loading
==================================================

Demo: Working with in-memory data
Total items: 8
  First item: {'id': 1, 'text': 'Machine learning is fascinating', 'category': 'tech'}
  Processed item: {'id': 1, 'text': 'Machine learning is fascinating', 'category': 'tech', 'text_length': 31}

==================================================
Part 2: Streaming Data Pattern
==================================================

Processing data as a stream:
  Tech item 1: Machine learning is fascinatin...
  Tech item 2: Python is a great language...
  Tech item 3: Neural networks are powerful...
  Tech items found: 3

==================================================
Part 3: Filter and Transform
==================================================

Filtered and transformed items:
  ID 1: 4 words, long=True
  ID 4: 5 words, long=False
  ID 6: 4 words, long=False

==================================================
Part 4: Batch Processing
==================================================

Processed batch 1: 3 items
Processed batch 2: 3 items
Processed batch 3: 2 items

==================================================
Part 5: Using Ember's Data API
==================================================

With real datasets, you would use:

1. Streaming (memory efficient):
   for item in data.stream('dataset_name'):
       process(item)

2. Loading (when you need all data):
   dataset = data.load('dataset_name', split='train')

3. Chaining operations:
   results = (data.stream('dataset')
             .filter(lambda x: x['score'] > 0.5)
             .transform(add_metadata)
             .first(100))

4. Custom data sources:
   data.register('my_data', MyDataSource())
   for item in data.stream('my_data'):
       process(item)

==================================================
Part 6: Optimizing with @jit
==================================================

Analyzing items with JIT optimization:

ID 1: Machine learning is fascinatin...
  Analysis: {'length': 31, 'words': 4, 'avg_word_length': 7.0, 'has_ml_terms': True, 'sentiment_hint': 'neutral'}

ID 2: I love cooking pasta...
  Analysis: {'length': 20, 'words': 4, 'avg_word_length': 4.25, 'has_ml_terms': False, 'sentiment_hint': 'positive'}

ID 3: The weather is beautiful today...
  Analysis: {'length': 30, 'words': 5, 'avg_word_length': 5.2, 'has_ml_terms': False, 'sentiment_hint': 'neutral'}

==================================================
Part 7: Real-World Data Pipeline
==================================================

Pipeline processed 8 items

Sample result:
  Original: Machine learning is fascinating
  Tokens: ['machine', 'learning', 'is', 'fascinating']
  Features: {'token_count': 4, 'unique_tokens': 4, 'avg_token_length': 7.0}

==================================================
✅ Key Takeaways
==================================================

1. Use streaming for large datasets:
   - data.stream() for memory efficiency
   - Process items one at a time

2. Simple functions for data processing:
   - No complex classes needed
   - Compose functions naturally

3. Optimize with @jit and vmap:
   - @jit for single-item processing
   - vmap for batch operations

4. Chain operations fluently:
   - Filter → Transform → Aggregate
   - Clean, readable pipelines

5. Register custom data sources:
   - Integrate any data format
   - Consistent API for all sources

Next: See streaming_data.py for advanced streaming patterns!
