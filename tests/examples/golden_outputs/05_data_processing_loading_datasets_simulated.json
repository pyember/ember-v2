{
  "version": "1.0",
  "example": "05_data_processing/loading_datasets.py",
  "execution_mode": "simulated",
  "sections": [
    {
      "header": "Part 1: Basic Data Loading",
      "output": ""
    },
    {
      "header": "Demo: Working with in-memory data",
      "output": "Total items: 8\n  First item: {'id': 1, 'text': 'Machine learning is fascinating', 'category': 'tech'}\n  Processed item: {'id': 1, 'text': 'Machine learning is fascinating', 'category': 'tech', 'text_length': 31}"
    },
    {
      "header": "Part 2: Streaming Data Pattern",
      "output": ""
    },
    {
      "header": "Processing data as a stream:",
      "output": "Tech item 1: Machine learning is fascinatin...\n  Tech item 2: Python is a great language...\n  Tech item 3: Neural networks are powerful...\n  Tech items found: 3"
    },
    {
      "header": "Part 3: Filter and Transform",
      "output": ""
    },
    {
      "header": "Filtered and transformed items:",
      "output": "ID 1: 4 words, long=True\n  ID 4: 5 words, long=False\n  ID 6: 4 words, long=False"
    },
    {
      "header": "Part 4: Batch Processing",
      "output": ""
    },
    {
      "header": "Processed batch 1: 3 items",
      "output": "Processed batch 2: 3 items\nProcessed batch 3: 2 items"
    },
    {
      "header": "Part 5: Using Ember's Data API",
      "output": ""
    },
    {
      "header": "With real datasets, you would use:",
      "output": "1. Streaming (memory efficient):\n   for item in data.stream('dataset_name'):\n       process(item)\n\n2. Loading (when you need all data):\n   dataset = data.load('dataset_name', split='train')\n\n3. Chaining operations:\n   results = (data.stream('dataset')\n             .filter(lambda x: x['score'] > 0.5)\n             .transform(add_metadata)\n             .first(100))\n\n4. Custom data sources:\n   data.register('my_data', MyDataSource())\n   for item in data.stream('my_data'):\n       process(item)"
    },
    {
      "header": "Part 6: Optimizing with @jit",
      "output": ""
    },
    {
      "header": "Analyzing items with JIT optimization:",
      "output": "ID 1: Machine learning is fascinatin...\n  Analysis: {'length': 31, 'words': 4, 'avg_word_length': 7.0, 'has_ml_terms': True, 'sentiment_hint': 'neutral'}\n\nID 2: I love cooking pasta...\n  Analysis: {'length': 20, 'words': 4, 'avg_word_length': 4.25, 'has_ml_terms': False, 'sentiment_hint': 'positive'}\n\nID 3: The weather is beautiful today...\n  Analysis: {'length': 30, 'words': 5, 'avg_word_length': 5.2, 'has_ml_terms': False, 'sentiment_hint': 'neutral'}"
    },
    {
      "header": "Part 7: Real-World Data Pipeline",
      "output": ""
    },
    {
      "header": "Pipeline processed 8 items",
      "output": "Sample result:\n  Original: Machine learning is fascinating\n  Tokens: ['machine', 'learning', 'is', 'fascinating']\n  Features: {'token_count': 4, 'unique_tokens': 4, 'avg_token_length': 7.0}"
    },
    {
      "header": "\u2705 Key Takeaways",
      "output": ""
    },
    {
      "header": "1. Use streaming for large datasets:",
      "output": "- data.stream() for memory efficiency\n   - Process items one at a time\n\n2. Simple functions for data processing:\n   - No complex classes needed\n   - Compose functions naturally\n\n3. Optimize with @jit and vmap:\n   - @jit for single-item processing\n   - vmap for batch operations\n\n4. Chain operations fluently:\n   - Filter \u2192 Transform \u2192 Aggregate\n   - Clean, readable pipelines\n\n5. Register custom data sources:\n   - Integrate any data format\n   - Consistent API for all sources\n\nNext: See streaming_data.py for advanced streaming patterns!"
    }
  ],
  "total_time": 1.48,
  "api_keys_required": [],
  "metrics": {
    "lines_of_code": 222,
    "api_calls": 5
  }
}