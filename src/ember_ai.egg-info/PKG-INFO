Metadata-Version: 2.4
Name: ember-ai
Version: 0.1.0
Summary: Compositional framework for building and orchestrating Compound AI Systems and Networks of Networks (NONs).
Author-email: Jared Quincy Davis <jaredq@cs.stanford.edu>
Maintainer-email: Ember Team <team@pyember.org>
License: MIT
Project-URL: repository, https://github.com/pyember/ember
Project-URL: documentation, https://docs.pyember.org
Project-URL: homepage, https://pyember.org
Keywords: AI,LLM,Networks of Networks,Machine Learning,Compound AI,Orchestration,AI Framework,LLM Orchestration,AI System Design
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Information Technology
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
Classifier: Operating System :: OS Independent
Requires-Python: <3.13,>=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pandas<2.2.0,>=1.0.0
Requires-Dist: numpy<1.27.0,>=1.21.0
Requires-Dist: pydantic>=2.7.4
Requires-Dist: pydantic-core>=2.18.4
Requires-Dist: pydantic-settings>=2.3.0
Requires-Dist: PyYAML>=6.0.1
Requires-Dist: typing_extensions>=4.12.2
Requires-Dist: openai>=1.57.2
Requires-Dist: anthropic>=0.40.0
Requires-Dist: google-generativeai>=0.8.3
Requires-Dist: aiohttp>=3.9.5
Requires-Dist: aiosignal>=1.3.1
Requires-Dist: httpx>=0.27.0
Requires-Dist: requests>=2.32.2
Requires-Dist: datasets>=2.20.0
Requires-Dist: tqdm>=4.67.1
Requires-Dist: tenacity>=9.0.0
Requires-Dist: cachetools>=5.4.0
Requires-Dist: dill>=0.3.8
Requires-Dist: prettytable>=3.12.0
Requires-Dist: annotated-types>=0.7.0
Requires-Dist: anyio>=4.4.0
Requires-Dist: attrs>=23.2.0
Requires-Dist: certifi>=2024.6.2
Requires-Dist: charset-normalizer>=3.3.2
Requires-Dist: packaging>=24.1
Requires-Dist: six>=1.16.0
Requires-Dist: idna>=3.7
Requires-Dist: urllib3<2.0.0,>=1.26.19
Requires-Dist: sniffio>=1.3.1
Provides-Extra: all
Requires-Dist: openai>=1.57.2; extra == "all"
Requires-Dist: anthropic>=0.40.0; extra == "all"
Requires-Dist: google-generativeai>=0.8.3; extra == "all"
Requires-Dist: scikit-learn>=1.6.0; extra == "all"
Requires-Dist: scipy>=1.13.1; extra == "all"
Requires-Dist: huggingface-hub>=0.26.5; extra == "all"
Requires-Dist: pyarrow>=16.1.0; extra == "all"
Requires-Dist: pyarrow-hotfix>=0.6; extra == "all"
Requires-Dist: matplotlib>=3.9.1; extra == "all"
Requires-Dist: prettytable>=3.12.0; extra == "all"
Requires-Dist: google-ai-generativelanguage>=0.6.6; extra == "all"
Requires-Dist: google-api-core>=2.19.1; extra == "all"
Requires-Dist: google-api-python-client>=2.139.0; extra == "all"
Requires-Dist: google-auth>=2.32.0; extra == "all"
Requires-Dist: google-auth-httplib2>=0.2.0; extra == "all"
Requires-Dist: googleapis-common-protos>=1.63.2; extra == "all"
Requires-Dist: pytest>=8.3.2; extra == "all"
Requires-Dist: pytest-asyncio>=0.23.4; extra == "all"
Requires-Dist: parameterized>=0.9.0; extra == "all"
Requires-Dist: pytest-cov>=4.1.0; extra == "all"
Requires-Dist: hypothesis>=6.99.0; extra == "all"
Requires-Dist: mutmut>=2.4.4; extra == "all"
Requires-Dist: tox>=4.11.4; extra == "all"
Requires-Dist: jupyterlab>=4.0.6; extra == "all"
Requires-Dist: ipykernel>=6.26.0; extra == "all"
Requires-Dist: black>=23.12.0; extra == "all"
Requires-Dist: isort>=5.12.0; extra == "all"
Requires-Dist: mypy>=1.7.1; extra == "all"
Requires-Dist: pylint>=3.0.2; extra == "all"
Requires-Dist: pre-commit>=3.5.0; extra == "all"
Requires-Dist: ruff>=0.1.6; extra == "all"
Requires-Dist: sphinx>=7.1.0; extra == "all"
Requires-Dist: sphinx-rtd-theme>=1.3.0; extra == "all"
Requires-Dist: nbsphinx>=0.9.3; extra == "all"
Requires-Dist: myst-parser>=2.0.0; extra == "all"
Requires-Dist: jupyter; extra == "all"
Requires-Dist: notebook; extra == "all"
Provides-Extra: minimal
Requires-Dist: openai>=1.57.2; extra == "minimal"
Provides-Extra: openai
Requires-Dist: openai>=1.57.2; extra == "openai"
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.40.0; extra == "anthropic"
Provides-Extra: google
Requires-Dist: google-generativeai>=0.8.3; extra == "google"
Requires-Dist: google-ai-generativelanguage>=0.6.6; extra == "google"
Requires-Dist: google-api-core>=2.19.1; extra == "google"
Requires-Dist: google-api-python-client>=2.139.0; extra == "google"
Requires-Dist: google-auth>=2.32.0; extra == "google"
Requires-Dist: google-auth-httplib2>=0.2.0; extra == "google"
Requires-Dist: googleapis-common-protos>=1.63.2; extra == "google"
Provides-Extra: allproviders
Requires-Dist: openai>=1.57.2; extra == "allproviders"
Requires-Dist: anthropic>=0.40.0; extra == "allproviders"
Requires-Dist: google-generativeai>=0.8.3; extra == "allproviders"
Provides-Extra: data
Requires-Dist: datasets>=2.20.0; extra == "data"
Requires-Dist: scikit-learn>=1.6.0; extra == "data"
Requires-Dist: scipy>=1.13.1; extra == "data"
Requires-Dist: huggingface-hub>=0.26.5; extra == "data"
Requires-Dist: pyarrow>=16.1.0; extra == "data"
Requires-Dist: pyarrow-hotfix>=0.6; extra == "data"
Provides-Extra: viz
Requires-Dist: matplotlib>=3.9.1; extra == "viz"
Requires-Dist: prettytable>=3.12.0; extra == "viz"
Provides-Extra: dev
Requires-Dist: pytest>=8.3.2; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23.4; extra == "dev"
Requires-Dist: parameterized>=0.9.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: hypothesis>=6.99.0; extra == "dev"
Requires-Dist: mutmut>=2.4.4; extra == "dev"
Requires-Dist: tox>=4.11.4; extra == "dev"
Requires-Dist: jupyterlab>=4.0.6; extra == "dev"
Requires-Dist: ipykernel>=6.26.0; extra == "dev"
Requires-Dist: black>=23.12.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.7.1; extra == "dev"
Requires-Dist: pylint>=3.0.2; extra == "dev"
Requires-Dist: pre-commit>=3.5.0; extra == "dev"
Requires-Dist: ruff>=0.1.6; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=7.1.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.3.0; extra == "docs"
Requires-Dist: nbsphinx>=0.9.3; extra == "docs"
Requires-Dist: myst-parser>=2.0.0; extra == "docs"
Requires-Dist: jupyter; extra == "docs"
Requires-Dist: notebook; extra == "docs"
Dynamic: license-file

<p align="center">
  <img src="docs/assets/logo_ember_icon@2x.png" alt="Ember Logo" width="150"/>
</p>
<p align="center">
  <img src="docs/assets/ember_workmark.svg" alt="Ember" width="350"/>
</p>

<p align="center">
<strong>Contributors</strong>
</p>

<p align="center">
This repository is in collaboration with the following early users, contributors, and reviewers:
</p>

<p align="center">
Jared Quincy Davis<sup>F,S</sup>, Marquita Ellis<sup>I</sup>, Diana Arroyo<sup>I</sup>, Pravein Govindan Kannan<sup>I</sup>, Paul Castro<sup>I</sup>, Siddharth Sharma<sup>F,S</sup>, Lingjiao Chen<sup>MS</sup>, Omar Khattab<sup>D,MT</sup>, Alan Zhu<sup>B</sup>, Parth Asawa<sup>B</sup>, Connor Chow<sup>B</sup>, Jason Lee<sup>B</sup>, Jay Adityanag Tipirneni<sup>B</sup>, Chad Ferguson<sup>B</sup>, Kathleen Ge<sup>B</sup>, Kunal Agrawal<sup>B</sup>, Rishab Bhatia<sup>B</sup>, Rohan Penmatcha<sup>B</sup>, Sai Kolasani<sup>B</sup>, Théo Jaffrelot Inizan<sup>B</sup>, Deepak Narayanan<sup>N</sup>, Long Fei<sup>F</sup>, Aparajit Raghavan<sup>F</sup>, Eyal Cidon<sup>F</sup>, Jacob Schein<sup>F</sup>, Prasanth Somasundar<sup>F</sup>, Boris Hanin<sup>F,P</sup>, James Zou<sup>S</sup>, Alex Dimakis<sup>B</sup>, Joey Gonzalez<sup>B</sup>, Peter Bailis<sup>G,S</sup>, Ion Stoica<sup>A,B,D</sup>, Matei Zaharia<sup>D,B</sup>
</p>

<p align="center">
<sup>F</sup> Foundry (MLFoundry), <sup>D</sup> Databricks, <sup>I</sup> IBM Research, <sup>S</sup> Stanford University, <sup>B</sup> UC Berkeley, <sup>MT</sup> MIT, <sup>N</sup> NVIDIA, <sup>MS</sup> Microsoft, <sup>A</sup> Anyscale, <sup>G</sup> Google, <sup>P</sup> Princeton
</p>

# <span style="color:#0366d6;">Ember</span>: A Compositional Framework for Compound AI Systems

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

## Ember in a Nutshell

Aspirationally, Ember is to Networks of Networks (NONs) Compound AI Systems development what PyTorch 
and XLA are to Neural Networks (NN) development. It's a compositional framework with both eager 
execution affordances and graph execution optimization capabilities. It enables users to compose 
complex NONs, and supports automatic parallelization and optimization of these. 


Ember's vision is to enable development of **compound AI systems composed of, one day, millions-billions of inference calls** and beyond. Simple constructs--like **best-of-N graphs**, **verifier-prover structures**, and **ensembles with “voting-based” aggregation**--work surprisingly well in many regimes. 

```python
# With Ember's "compact notation" it is one line to build a simple parallel system with 101 GPT-4o instances synthesized by Claude
system = non.build_graph(["101:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"])  # Automatically parallelized
result = system(query="What's the most effective climate change solution?")
```

This led us to believe that there is a rich architecture space for constructing and optimizing what we call “networks of networks” graphs, or **NONs**. This is analogous to how neural network architecture research uncovered many emergent properties of systems composed of simple artificial neurons. It would be frictionful to conduct NN research if we had to implement architectures from scratch via for-loops or implement bespoke libraries for vectorization and efficient execution. Similarly, it can be challenging at present to compose NON architectures of many calls, despite the **rapidly falling cost-per-token of intelligence**.

Ember's goal is to help unlock research and practice along this new frontier. 

## Documentation & Examples

- [Architecture Overview](ARCHITECTURE.md)
- [Quick Start Guide](QUICKSTART.md)
- [LLM Specifications](LLM_SPECIFICATIONS.md)
- [Model Registry Guide](docs/quickstart/model_registry.md)
- [Operators Guide](docs/quickstart/operators.md)
- [NON Patterns](docs/quickstart/non.md)
- [Data Processing](docs/quickstart/data.md)
- [Configuration](docs/quickstart/configuration.md)
- [Examples Directory](src/ember/examples)

## Simple Example: Ensemble Reasoning with Automatic Parallelization

```python
class QueryInput(EmberModel):
    query: str
    
class ConfidenceOutput(EmberModel):
    answer: str
    confidence: float

class ReasonerSpec(Specification):
    input_model = QueryInput
    structured_output = ConfidenceOutput

@jit # Autonomically optimize execution with JIT compilation (e.g. TopoSort with Parallel Dispatch)
class EnsembleReasoner(Operator[QueryInput, ConfidenceOutput]):
    specification = ReasonerSpec()
    
    def __init__(self, width: int = 3):
        self.ensemble = non.UniformEnsemble(
            num_units=width,
            model_name="openai:gpt-4o",
            temperature=0.7
        )
        
        self.judge = non.JudgeSynthesis(
            model_name="anthropic:claude-3-5-sonnet",
        )
    
    def forward(self, *, inputs: QueryInput) -> ReasonedOutput:
        # These operations are automatically parallelized by Ember's XCS system
        ensemble_result = self.ensemble(query=inputs.query)
        
        synthesis = self.judge(
            query=inputs.query,
            responses=ensemble_result["responses"]
        )
        
        return ConfidenceOutput(
            answer=synthesis["final_answer"],
            confidence=float(synthesis.get("confidence", 0.0))
        )

# Use it like any Python function
compound_system = EnsembleReasoner()
result = compound_system(query="What causes the northern lights?")
print(f"Answer: {result.answer}")
print(f"Confidence: {result.confidence:.2f}")

# Alternatively, build the same pipeline with compact notation
pipeline = non.build_graph(["3:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.2"])
result = pipeline(query="What causes the northern lights?")
```

## Compact Notation

Ember's compact notation allows expression of complex AI architectures in minimal code:

```python
# Compact notation: "count:type:model:temperature" - each component precisely specified

# BASIC: Single-line systems with automatic parallelization
basic = non.build_graph(["7:E:gpt-4o:0.7"])                             # 7-model ensemble
voting = non.build_graph(["7:E:gpt-4o:0.7", "1:M"])                     # With majority voting
judged = non.build_graph(["7:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"])   # With judge synthesis

# STANDARD API: Equivalent to compact notation but with explicit objects
standard_system = non.Sequential(operators=[
    non.UniformEnsemble(num_units=7, model_name="gpt-4o", temperature=0.7),
    non.JudgeSynthesis(model_name="claude-3-5-sonnet", temperature=0.0)
])

# ADVANCED: Reusable components for complex architectures
components = {
    # Define building blocks once, reuse everywhere
    "reasoning": ["3:E:gpt-4o:0.7", "1:V:gpt-4o:0.0"],           # Verification pipeline
    "research": ["3:E:claude-3-5-sonnet:0.5", "1:V:claude-3-5-sonnet:0.0"]  # Different models
}

# Build sophisticated multi-branch architecture in just 4 lines
advanced = non.build_graph([
    "$reasoning",                     # First branch: reasoning with verification
    "$research",                      # Second branch: research with verification
    "1:J:claude-3-5-opus:0.0"         # Final synthesis of both branches
], components=components)             # Automatically optimized for parallel execution

# HORIZONTAL SCALING: Systematically explore scaling behavior
systems = {
    # Scaling with MostCommon aggregation
    "width_3_voting": non.build_graph(["3:E:gpt-4o:0.7", "1:M"]), 
    "width_7_voting": non.build_graph(["7:E:gpt-4o:0.7", "1:M"]),
    "width_11_voting": non.build_graph(["11:E:gpt-4o:0.7", "1:M"]),
    
    # Scaling with judge synthesis
    "width_3_judge": non.build_graph(["3:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"]),
    "width_7_judge": non.build_graph(["7:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"]),
    "width_11_judge": non.build_graph(["11:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"]),
}

# Execute with full parallelism (XCS optimizes the execution graph automatically)
query = "What's the most effective climate change solution?"
results = {name: system(query=query) for name, system in systems.items()}
```

## Core Elements

1. **Composable Operators with Rigorous Specification**: Build reliable compound AI systems from 
   type-safe, reusable components with validated inputs and outputs
2. **Automatic Parallelization**: Independent operations are automatically executed concurrently 
   across a full computational graph
3. **XCS Optimization Framework**: "Accelerated Compound Systems" Just-in-time tracing and execution optimization with multiple strategies (trace, structural, enhanced). XCS is inspired by XLA, but intended more for accelerating compound systems vs. linear algebra operations, tuned for models and dicts, vs for vectors and numerical computation.
4. **Multi-Provider Support**: Unified API across OpenAI, Anthropic, Claude, Gemini, and more 
   with standardized usage tracking
5. **Transformation System**: Function transformations for vectorization (vmap), parallelization (pmap), and device sharding (mesh), with a composable interface for building complex transformations

## XCS Architecture

The Accelerated Compound Systems (XCS) module provides a computational graph-based system for building, optimizing, and executing complex operator pipelines:

1. **Unified JIT System**: Multiple compilation strategies under a consistent interface:
   - `trace`: Traditional execution tracing
   - `structural`: Structure-based analysis
   - `enhanced`: Improved parallelism detection and code analysis

2. **Scheduler Framework**: Pluggable scheduler implementations for different execution patterns:
   - `sequential`: Serial execution for debugging and determinism
   - `parallel`: Thread-based parallel execution
   - `wave`: Execution wave scheduling for optimal parallelism
   - `topological`: Dependency-based execution ordering

3. **Transform System**: High-level operations for data and computation transformations:
   - `vmap`: Vectorized mapping for batch processing
   - `pmap`: Parallel mapping across multiple workers
   - `mesh`: Device mesh-based sharding for multi-device execution

4. **Dependency Analysis**: Automatic extraction of dependencies between operations:
   - Transitive closure calculation for complete dependency mapping
   - Topological sorting with cycle detection
   - Execution wave computation for parallel scheduling

## Installation

Ember uses [uv](https://github.com/astral-sh/uv) as its recommended package manager for significantly faster installations and dependency resolution.

```bash
# First, install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh  # macOS/Linux
# or 
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"  # Windows
# or
pip install uv  # Any platform

# Quick install using uv (recommended)
uv pip install ember-ai

# Run examples directly with uv (no activation needed)
uv run python -c "import ember; print(ember.__version__)"

# Install from source for development
git clone https://github.com/pyember/ember.git
cd ember
uv pip install -e ".[dev]"

# Traditional pip installation (alternative, slower)
pip install ember-ai
```

For detailed installation instructions, troubleshooting, and environment management, see our [Installation Guide](INSTALLATION_GUIDE.md).

## Model Registry & Provider Integration

Access models from any provider through a unified interface:

```python
from ember import initialize_ember
from ember.api.models import ModelEnum

# Initialize with multiple providers
service = initialize_ember(usage_tracking=True)

# Access models from different providers with the same API
response = service(ModelEnum.gpt_4o, "What is quantum computing?")
print(response.data)

# Track usage across providers
usage = service.usage_service.get_total_usage()
print(f"Total cost: ${usage.cost:.4f}")
```

## NON Patterns & Ensembling

Build compound AI system architectures using the Network of Networks (NON) pattern with pre-built components:

```python
from ember.api import non

# Standard API: Create a verification pipeline of ensemble→judge→verifier
pipeline = non.Sequential(operators=[
    # 1. Ensemble of 5 model instances running in parallel
    non.UniformEnsemble(
        num_units=5, 
        model_name="openai:gpt-4o-mini",
        temperature=0.7
    ),
    
    # 2. Judge to synthesize the ensemble responses
    non.JudgeSynthesis(
        model_name="anthropic:claude-3-5-sonnet",
        temperature=0.2
    ),
    
    # 3. Verifier for quality control and fact-checking
    non.Verifier(
        model_name="anthropic:claude-3-5-haiku",
        temperature=0.0
    )
])

# Alternatively, create the same pipeline with compact notation
pipeline = non.build_graph([
    "5:E:gpt-4o-mini:0.7",        # Ensemble with 5 instances
    "1:J:claude-3-5-sonnet:0.2",  # Judge synthesis
    "1:V:claude-3-5-haiku:0.0"    # Verification
])

# Build advanced architectures like NestedNetwork from example_architectures.py
# Define reusable SubNetwork component
components = {
    "sub": ["2:E:gpt-4o:0.0", "1:V:gpt-4o:0.0"]  # Ensemble → Verifier
}

# Create a NestedNetwork with identical structure to the OOP implementation
nested = non.build_graph([
    "$sub",                # First SubNetwork branch
    "$sub",                # Second SubNetwork branch
    "1:J:gpt-4o:0.0"       # Judge to synthesize results
], components=components)

# Extend with custom operator types
custom_registry = non.OpRegistry.create_standard_registry()
custom_registry.register(
    "CE",  # Custom ensemble type
    lambda count, model, temp: non.Sequential(operators=[
        non.UniformEnsemble(num_units=count, model_name=model, temperature=temp),
        non.MostCommon()  # Auto-aggregation 
    ])
)

# Use custom operators
advanced = non.build_graph(["3:CE:gpt-4o:0.7"], type_registry=custom_registry)

# Execute with a single call
result = pipeline(query="What causes tsunamis?")
```

## Graph Optimization & Execution

Ember's XCS system provides JAX/XLA-inspired tracing, transformation, and automatic parallelization:

```python
from ember.xcs import jit, execution_options, vmap, pmap, compose, explain_jit_selection
from ember.api.operators import Operator

# Basic JIT compilation with automatic strategy selection
@jit
class SimplePipeline(Operator):
    # ... operator implementation ...

# JIT with explicit mode selection
@jit(mode="enhanced")
class ComplexPipeline(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()
        self.op3 = SubOperator3()
    
    def forward(self, *, inputs):
        # These operations will be automatically parallelized
        result1 = self.op1(inputs=inputs)
        result2 = self.op2(inputs=inputs)
        
        # Combine the parallel results
        combined = self.op3(inputs={"r1": result1, "r2": result2})
        return combined

# Configure execution parameters
with execution_options(scheduler="wave", max_workers=4):
    result = pipeline(query="Complex question...") 

# Get explanation for JIT strategy selection
explanation = explain_jit_selection(pipeline)
print(f"JIT strategy: {explanation['strategy']}")
print(f"Rationale: {explanation['rationale']}")

# Vectorized mapping for batch processing
batch_processor = vmap(my_operator)
batch_results = batch_processor(inputs={"data": [item1, item2, item3]})

# Parallel execution across multiple workers
parallel_processor = pmap(my_operator, num_workers=4)
parallel_results = parallel_processor(inputs=complex_data)

# Compose transformations (vectorization + parallelism)
pipeline = compose(vmap(batch_size=32), pmap(num_workers=4))(my_operator)
```

## Data Handling & Evaluation

Ember provides a comprehensive data processing and evaluation framework with pre-built datasets and metrics:

```python
from ember.api.data import DatasetBuilder
from ember.api.eval import EvaluationPipeline, Evaluator

# Load a dataset with the builder pattern
dataset = (DatasetBuilder()
    .from_registry("mmlu")  # Use a registered dataset
    .subset("physics")      # Select a specific subset
    .split("test")          # Choose the test split
    .sample(100)            # Random sample of 100 items
    .transform(              # Apply transformations
        lambda x: {"query": f"Question: {x['question']}"} 
    )
    .build())

# Create a comprehensive evaluation pipeline
eval_pipeline = EvaluationPipeline([
    # Standard metrics
    Evaluator.from_registry("accuracy"),
    Evaluator.from_registry("response_quality"),
    
    # Custom evaluation metrics
    Evaluator.from_function(
        lambda prediction, reference: {
            "factual_accuracy": score_factual_content(prediction, reference)
        }
    )
])

# Evaluate a model or operator
results = eval_pipeline.evaluate(my_model, dataset)
print(f"Accuracy: {results['accuracy']:.2f}")
print(f"Response Quality: {results['response_quality']:.2f}")
print(f"Factual Accuracy: {results['factual_accuracy']:.2f}")
```

## License

Ember is released under the [MIT License](LICENSE).
