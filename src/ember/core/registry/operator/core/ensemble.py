"""
Parallel Ensemble Pattern Implementation

This module implements the Ensemble pattern, a foundational distributed inference pattern
that enables parallel execution of multiple language model instances. This pattern addresses
several critical needs in Compound AI Systems applications:

1. Statistical Robustness: Multiple "independent" inferences reduce variance in outputs
2. Throughput Optimization: Concurrent execution maximizes utilization of available resources, or rate-limits
3. Entropy and diversity of perspectives: Different models or configurations can provide complementary insights

The pattern serves as a foundational building block for more complex NON patterns
and reliability-focused LLM applications.
"""

from __future__ import annotations

from typing import List, Union, Any, Optional

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types import EmberModel


class EnsembleOperatorInputs(EmberModel):
    """
    Structured input model for ensemble inference operations.

    This model defines the minimal input contract for ensemble operations,
    focusing on the core query text while allowing the specification to
    handle rendering details. The model is intentionally minimalist to
    enable maximum flexibility in ensemble configurations.

    Attributes:
        query: The primary text query to be sent to all language models
              in the ensemble. This text will be rendered according to
              the operator's specification template if one is defined.
    """

    query: str


class EnsembleOperatorOutputs(EmberModel):
    """
    Structured output model for ensemble inference results.

    This model provides a standardized container for the parallel responses
    generated by the ensemble. The ordered list preserves the relationship
    between responses and their source models, enabling downstream operators
    to apply model-specific weighting or processing if needed.

    Notable design considerations:
    - Maintains original response ordering for reproducibility
    - Preserves raw text outputs for maximum flexibility
    - Simple structure facilitates easy aggregation and analysis

    Attributes:
        responses: Ordered list of text responses from the language models
                 in the ensemble. Each element corresponds to the output
                 from one model instance, preserving the original order of
                 the models provided to the operator.
    """

    responses: List[str]


class EnsembleOperator(Operator[EnsembleOperatorInputs, EnsembleOperatorOutputs]):
    """
    Executes the same query across multiple language models in parallel.

    Sends an identical prompt to each model in the ensemble and collects all responses.
    This enables multiple independent samples from language models, which can
    be used for robustness, consensus, or diversity of outputs.

    The execution is implicitly parallel, with each model potentially running
    concurrently depending on the implementation.
    """

    specification: Specification = Specification(
        input_model=EnsembleOperatorInputs, structured_output=EnsembleOperatorOutputs
    )

    def __init__(
        self, 
        *, 
        models: List[Any],  # List of callables
        **kwargs
    ) -> None:
        """
        Initializes the ensemble with a collection of language models.

        The constructor accepts a list of callable models. Each model should be
        a function or object that can be called with a prompt string and returns
        a response object with a .text attribute.

        The implementation preserves the order of the provided models,
        ensuring deterministic execution and reproducible results.

        Args:
            models: Collection of callable models to execute in parallel.
                   Each should accept a prompt string and return a response.
            **kwargs: Additional parameters passed to the operator
            
        Examples:
            # Usage with pre-configured models from the API layer
            from ember.api import models
            ensemble = EnsembleOperator(models=[
                models.instance("gpt-4", temperature=0.7),
                models.instance("claude-3", temperature=0.9),
                models.instance("gpt-3.5-turbo", temperature=0.5)
            ])
        """
        super().__init__(**kwargs)
        self.models = models

    def forward(self, *, inputs: EnsembleOperatorInputs) -> EnsembleOperatorOutputs:
        """
        Executes the query across all language models.

        Args:
            inputs: Contains the query to send to all models.

        Returns:
            Contains all model responses in an ordered list matching
            the original models order.
        """
        rendered_prompt: str = self.specification.render_prompt(inputs=inputs)
        responses: List[str] = []
        
        # Execute each model and collect responses
        for model in self.models:
            response = model(rendered_prompt)
            # Handle both old LMModule (returns string) and new models API (returns object with .text)
            response_text = response.text if hasattr(response, 'text') else response
            responses.append(response_text)
            
        return {"responses": responses}
