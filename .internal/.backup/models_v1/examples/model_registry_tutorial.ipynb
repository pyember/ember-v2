{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ember Model Registry Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide to using Ember's Model Registry system. You'll learn how to:\n",
    "\n",
    "1. Initialize the model registry\n",
    "2. Configure and use different LLM providers\n",
    "3. Access models through different API patterns\n",
    "4. Track usage and manage costs\n",
    "5. Create custom providers\n",
    "6. Handle advanced scenarios like streaming and concurrency\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "First, let's import the necessary components and initialize the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Import Ember components\n",
    "from ember.core.registry.model.initialization import initialize_registry\n",
    "from ember.core.registry.model.base.services.model_service import ModelService, create_model_service\n",
    "from ember.core.registry.model.base.services.usage_service import UsageService\n",
    "from ember.core.registry.model.config.model_enum import ModelEnum\n",
    "from ember.api import models  # High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Environment Setup\n",
    "\n",
    "To use LLM providers, you need to set up your API keys. In a production environment, these would be set as environment variables or in a configuration file. For this notebook, we'll set them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up API keys (replace with your own or use environment variables)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"  # Replace with your key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_ANTHROPIC_API_KEY\"  # Replace with your key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"  # Replace with your key\n",
    "\n",
    "# For safety in the notebook, we'll check if keys are actually set\n",
    "providers_available = {\n",
    "    \"OpenAI\": bool(os.environ.get(\"OPENAI_API_KEY\")),\n",
    "    \"Anthropic\": bool(os.environ.get(\"ANTHROPIC_API_KEY\")),\n",
    "    \"Google/Deepmind\": bool(os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "}\n",
    "\n",
    "print(\"Available providers:\")\n",
    "for provider, available in providers_available.items():\n",
    "    print(f\"- {provider}: {'✅' if available else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize the Model Registry\n",
    "\n",
    "Now we'll initialize the model registry, which will discover available models from configured providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the registry with auto-discovery enabled\n",
    "registry = initialize_registry(auto_discover=True)\n",
    "\n",
    "# Create a model service with usage tracking\n",
    "usage_service = UsageService()\n",
    "model_service = create_model_service(\n",
    "    registry=registry,\n",
    "    usage_service=usage_service\n",
    ")\n",
    "\n",
    "# List available models\n",
    "available_models = registry.list_models()\n",
    "print(f\"Discovered {len(available_models)} models:\")\n",
    "for model in available_models[:10]:  # Show first 10 models\n",
    "    print(f\"- {model}\")\n",
    "if len(available_models) > 10:\n",
    "    print(f\"... and {len(available_models) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Usage Patterns\n",
    "\n",
    "Ember provides multiple ways to interact with models. Let's explore the different access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Direct Model Service Usage\n",
    "\n",
    "The most basic approach is to use the model service directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a model that's available based on our environment check\n",
    "model_id = None\n",
    "if providers_available[\"OpenAI\"]:\n",
    "    model_id = \"openai:gpt-4o\"\n",
    "elif providers_available[\"Anthropic\"]:\n",
    "    model_id = \"anthropic:claude-3-5-sonnet\"\n",
    "elif providers_available[\"Google/Deepmind\"]:\n",
    "    model_id = \"deepmind:gemini-1.5-pro\"\n",
    "else:\n",
    "    print(\"No providers available. Using mock model for demonstration.\")\n",
    "    model_id = \"mock:text-model\"\n",
    "\n",
    "# Use the model service to invoke the model\n",
    "if model_id:\n",
    "    response = model_service.invoke_model(\n",
    "        model_id=model_id,\n",
    "        prompt=\"What is the Ember framework?\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(f\"Response from {model_id}:\\n{response.data}\\n\")\n",
    "    print(f\"Usage: {response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Function-Style API\n",
    "\n",
    "For a more concise approach, you can use the function-style API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Use the high-level API\n",
    "try:\n",
    "    if providers_available[\"OpenAI\"]:\n",
    "        response = models.openai.gpt4o(\"What are the benefits of using Networks of Networks (NONs)?\")\n",
    "        print(f\"OpenAI GPT-4o response:\\n{response.data}\\n\")\n",
    "    \n",
    "    if providers_available[\"Anthropic\"]:\n",
    "        response = models.anthropic.claude_3_5_sonnet(\"How does Ember help with LLM orchestration?\")\n",
    "        print(f\"Anthropic Claude response:\\n{response.data}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error using function-style API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Enum-Based Access\n",
    "\n",
    "For type safety and IDE support, you can use enum-based access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Use enum-based access\n",
    "try:\n",
    "    response = model_service(ModelEnum.GPT4O, \"What are model enumerations good for?\")\n",
    "    print(f\"Response using enum access:\\n{response.data}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error using enum-based access: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provider Integration\n",
    "\n",
    "Let's explore how Ember integrates with different providers and how you can configure provider-specific parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Provider-Specific Parameters\n",
    "\n",
    "Each provider has specific parameters you can configure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# OpenAI-specific parameters\n",
    "if providers_available[\"OpenAI\"]:\n",
    "    response = model_service.invoke_model(\n",
    "        model_id=\"openai:gpt-4o\",\n",
    "        prompt=\"Generate a creative story idea.\",\n",
    "        temperature=0.9,  # Higher creativity\n",
    "        max_tokens=200,\n",
    "        provider_params={\n",
    "            \"top_p\": 0.95,\n",
    "            \"presence_penalty\": 0.5,\n",
    "            \"frequency_penalty\": 0.5\n",
    "        }\n",
    "    )\n",
    "    print(f\"OpenAI response with custom parameters:\\n{response.data}\\n\")\n",
    "\n",
    "# Anthropic-specific parameters\n",
    "if providers_available[\"Anthropic\"]:\n",
    "    response = model_service.invoke_model(\n",
    "        model_id=\"anthropic:claude-3-5-sonnet\",\n",
    "        prompt=\"Generate a creative story idea.\",\n",
    "        temperature=0.9,\n",
    "        max_tokens=200,\n",
    "        provider_params={\n",
    "            \"top_k\": 40,\n",
    "            \"top_p\": 0.95,\n",
    "            \"stop_sequences\": [\"THE END\"]\n",
    "        }\n",
    "    )\n",
    "    print(f\"Anthropic response with custom parameters:\\n{response.data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Information and Metadata\n",
    "\n",
    "You can access detailed information about registered models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get model info for a specific model\n",
    "if model_id:\n",
    "    model_info = registry.get_model_info(model_id)\n",
    "    if model_info:\n",
    "        print(f\"Model ID: {model_info.id}\")\n",
    "        print(f\"Model Name: {model_info.name}\")\n",
    "        print(f\"Provider: {model_info.provider.name}\")\n",
    "        print(f\"Input Cost: ${model_info.cost.input_cost_per_thousand/1000:.6f} per token\")\n",
    "        print(f\"Output Cost: ${model_info.cost.output_cost_per_thousand/1000:.6f} per token\")\n",
    "        \n",
    "        if model_info.provider.custom_args:\n",
    "            print(\"\\nProvider Custom Args:\")\n",
    "            for key, value in model_info.provider.custom_args.items():\n",
    "                print(f\"- {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"No model info found for {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Discovery and Registration\n",
    "\n",
    "Let's explore how the model discovery process works and how to manually register models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Manual Model Registration\n",
    "\n",
    "You can manually register models with the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ember.core.registry.model.base.schemas.model_info import ModelInfo, ProviderInfo\n",
    "from ember.core.registry.model.base.schemas.cost import ModelCost, RateLimit\n",
    "\n",
    "# Create a custom model info\n",
    "custom_model_info = ModelInfo(\n",
    "    id=\"custom:my-model\",\n",
    "    name=\"Custom Model\",\n",
    "    provider=ProviderInfo(\n",
    "        name=\"Custom\",\n",
    "        default_api_key=\"mock-api-key\"\n",
    "    ),\n",
    "    cost=ModelCost(\n",
    "        input_cost_per_thousand=0.001,\n",
    "        output_cost_per_thousand=0.002\n",
    "    ),\n",
    "    rate_limit=RateLimit(\n",
    "        tokens_per_minute=100000,\n",
    "        requests_per_minute=3000\n",
    "    )\n",
    ")\n",
    "\n",
    "# Register the custom model\n",
    "try:\n",
    "    registry.register_model(custom_model_info)\n",
    "    print(f\"Successfully registered custom model: {custom_model_info.id}\")\n",
    "    \n",
    "    # Verify it's in the list of models\n",
    "    if custom_model_info.id in registry.list_models():\n",
    "        print(\"Custom model is now in the registry\")\n",
    "except Exception as e:\n",
    "    print(f\"Error registering custom model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Discovery Process\n",
    "\n",
    "Let's trigger a model discovery to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trigger model discovery\n",
    "print(\"Initiating model discovery...\")\n",
    "new_models = registry.discover_models()\n",
    "\n",
    "if new_models:\n",
    "    print(f\"Discovered {len(new_models)} new models:\")\n",
    "    for model in new_models:\n",
    "        print(f\"- {model}\")\n",
    "else:\n",
    "    print(\"No new models discovered (all models were already registered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Usage Tracking\n",
    "\n",
    "One of the powerful features of Ember's model registry is built-in usage tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make some model calls to generate usage data\n",
    "if model_id:\n",
    "    prompts = [\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"What are the ethical implications of AI?\"\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        try:\n",
    "            print(f\"Generating response {i}/3...\")\n",
    "            model_service.invoke_model(\n",
    "                model_id=model_id,\n",
    "                prompt=prompt,\n",
    "                max_tokens=100  # Keep responses short for demo\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking model: {e}\")\n",
    "    \n",
    "    # Get usage statistics\n",
    "    total_usage = usage_service.get_total_usage()\n",
    "    print(\"\\nUsage Statistics:\")\n",
    "    print(f\"Total Tokens: {total_usage.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {total_usage.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {total_usage.completion_tokens}\")\n",
    "    print(f\"Estimated Cost: ${total_usage.cost:.6f}\")\n",
    "    \n",
    "    # Get usage by model\n",
    "    usage_by_model = usage_service.get_usage_by_model()\n",
    "    print(\"\\nUsage by Model:\")\n",
    "    for model_id, usage in usage_by_model.items():\n",
    "        print(f\"- {model_id}: {usage.total_tokens} tokens, ${usage.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Provider\n",
    "\n",
    "Advanced users can create custom providers to integrate with other LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from ember.core.registry.model.providers.base_provider import BaseProviderModel, BaseChatParameters\n",
    "from ember.core.registry.model.base.schemas.chat_schemas import ChatRequest, ChatResponse\n",
    "from ember.core.registry.model.base.schemas.usage import UsageStats\n",
    "\n",
    "# Define a simple custom provider\n",
    "class CustomProviderModel(BaseProviderModel):\n",
    "    # Set the provider name (required for discovery and registration)\n",
    "    PROVIDER_NAME = \"Custom\"\n",
    "    \n",
    "    def create_client(self) -> Any:\n",
    "        # In a real provider, you would initialize your API client here\n",
    "        # For this example, we'll just return a dummy client\n",
    "        return {\"api_key\": self.model_info.provider.default_api_key}\n",
    "    \n",
    "    def forward(self, request: ChatRequest) -> ChatResponse:\n",
    "        # In a real provider, you would call your API here\n",
    "        # For this example, we'll just echo the prompt\n",
    "        response_text = f\"[CUSTOM PROVIDER] You asked: {request.prompt}\\n\\nThis is a simulated response from {self.model_info.name}.\"\n",
    "        \n",
    "        # Calculate basic usage stats\n",
    "        prompt_tokens = len(request.prompt.split())\n",
    "        completion_tokens = len(response_text.split())\n",
    "        \n",
    "        usage = UsageStats(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            completion_tokens=completion_tokens,\n",
    "            total_tokens=prompt_tokens + completion_tokens,\n",
    "            cost_usd=0.0  # Simulated cost\n",
    "        )\n",
    "        \n",
    "        return ChatResponse(\n",
    "            data=response_text,\n",
    "            raw_output={\"simulated\": True},\n",
    "            usage=usage\n",
    "        )\n",
    "\n",
    "# Register the custom provider with the model factory\n",
    "from ember.core.registry.model.base.registry.factory import ModelFactory\n",
    "\n",
    "try:\n",
    "    ModelFactory.register_custom_provider(\n",
    "        provider_name=\"Custom\",\n",
    "        provider_class=CustomProviderModel\n",
    "    )\n",
    "    print(\"Successfully registered custom provider\")\n",
    "    \n",
    "    # Now we can use our custom provider\n",
    "    response = model_service.invoke_model(\n",
    "        model_id=\"custom:my-model\",\n",
    "        prompt=\"This is a test of my custom provider\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResponse from custom provider:\\n{response.data}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with custom provider: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics\n",
    "\n",
    "Let's explore some more advanced topics like error handling and concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Error Handling\n",
    "\n",
    "Proper error handling is essential for robust applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Attempt to invoke a non-existent model\n",
    "try:\n",
    "    response = model_service.invoke_model(\n",
    "        model_id=\"nonexistent:model\",\n",
    "        prompt=\"This should fail\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error: {e}\")\n",
    "    print(\"This is good! The error was caught properly.\")\n",
    "\n",
    "# Error handling with invalid parameters\n",
    "try:\n",
    "    response = model_service.invoke_model(\n",
    "        model_id=model_id,\n",
    "        prompt=\"This should fail\",\n",
    "        temperature=3.0  # Invalid temperature (should be between 0 and 2)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nExpected error with invalid parameters: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Concurrency\n",
    "\n",
    "Ember's model registry is designed to be thread-safe for concurrent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def invoke_model(prompt):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = model_service.invoke_model(\n",
    "            model_id=model_id,\n",
    "            prompt=prompt,\n",
    "            max_tokens=50  # Keep responses short for demo\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response.data[:50] + \"...\",  # Truncate for display\n",
    "            \"duration\": duration,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# Set up concurrent requests\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is the formula for water?\",\n",
    "    \"What is the tallest mountain in the world?\"\n",
    "]\n",
    "\n",
    "if model_id:\n",
    "    print(\"Executing concurrent model invocations...\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(invoke_model, prompt) for prompt in prompts]\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    print(\"\\nConcurrent invocation results:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        if result[\"success\"]:\n",
    "            print(f\"Request {i}: {result['prompt']}\")\n",
    "            print(f\"Response: {result['response']}\")\n",
    "            print(f\"Duration: {result['duration']:.2f} seconds\\n\")\n",
    "        else:\n",
    "            print(f\"Request {i}: {result['prompt']}\")\n",
    "            print(f\"Error: {result['error']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices\n",
    "\n",
    "Let's wrap up by discussing some best practices for using the model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Using Ember Model Registry\n",
    "\n",
    "1. **API Key Management**\n",
    "   - Store API keys in environment variables or secure configuration\n",
    "   - Rotate keys regularly according to provider recommendations\n",
    "   - Use different keys for development, testing, and production\n",
    "\n",
    "2. **Cost Management**\n",
    "   - Leverage usage tracking to monitor costs\n",
    "   - Set token limits appropriate for your use case\n",
    "   - Use lower-cost models for tasks that don't require high capability\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Always implement proper error handling\n",
    "   - Have fallback strategies for API outages\n",
    "   - Implement retry logic with exponential backoff\n",
    "\n",
    "4. **Performance Optimization**\n",
    "   - Use concurrent requests for independent operations\n",
    "   - Implement caching for common queries\n",
    "   - Set appropriate timeouts to prevent hanging requests\n",
    "\n",
    "5. **Testing**\n",
    "   - Create mock providers for testing\n",
    "   - Use deterministic responses in test environments\n",
    "   - Set up test fixtures with known inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've explored the Ember Model Registry's capabilities:\n",
    "\n",
    "1. We learned how to initialize and configure the model registry\n",
    "2. We explored different API patterns for model invocation\n",
    "3. We saw how to work with provider-specific parameters\n",
    "4. We demonstrated model discovery and manual registration\n",
    "5. We tracked and visualized usage statistics\n",
    "6. We created a custom provider implementation\n",
    "7. We handled errors and concurrent operations\n",
    "\n",
    "The Ember Model Registry provides a flexible and powerful foundation for building LLM applications with multiple providers. It simplifies provider integration, standardizes model access, and provides essential features like usage tracking and error handling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}