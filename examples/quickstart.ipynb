{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "welcome",
   "metadata": {},
   "source": [
    "# Ember Quickstart: From Zero to Advanced AI Systems\n",
    "\n",
    "Welcome to Ember - a principled framework for building production AI systems. This notebook takes you through all key concepts, from basic model calls to advanced JAX integration with learnable components.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Setting up and making your first LLM call\n",
    "2. Working with the Models API for multi-provider orchestration\n",
    "3. Efficient data processing with streaming-first design\n",
    "4. Building composable AI systems with Operators\n",
    "5. Creating ensemble and judge systems for evaluation\n",
    "6. Zero-config optimization with XCS\n",
    "7. Advanced: JAX gradients on mixed structures with learnable routing\n",
    "\n",
    "**Time to complete:** ~30 minutes\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Environment Check\n",
    "\n",
    "First, let's ensure Ember is properly installed and configured. If you haven't run the setup wizard yet, we'll guide you through it."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "# Check if Ember is installed\ntry:\n    import ember\n    print(f\"✅ Ember {ember.__version__} is installed\")\nexcept ImportError:\n    print(\"❌ Ember not found. Please install with: pip install ember-ai\")\n    print(\"\\n💡 Tip: After installation, run 'uv run ember setup' for interactive configuration\")\n    raise\n\n# Check for API keys\nimport os\napi_keys_found = []\nfor key in [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\", \"GOOGLE_API_KEY\"]:\n    if os.getenv(key):\n        api_keys_found.append(key.replace(\"_API_KEY\", \"\"))\n\nif api_keys_found:\n    print(f\"✅ Found API keys for: {', '.join(api_keys_found)}\")\nelse:\n    print(\"⚠️  No API keys found. Run 'uv run ember setup' to configure\")\n\n# Import key modules\nfrom ember.api import models, operators, op, data, stream\nfrom ember.api.xcs import jit\nimport jax\nimport jax.numpy as jnp\nimport optax\nimport time\n\nprint(\"\\n✅ All imports successful! Let's begin.\")"
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Module 1: Models API - Multi-Provider LLM Orchestration\n",
    "\n",
    "Ember provides a unified interface for OpenAI, Anthropic, Google, and more. Let's explore available models and key patterns."
   ]
  },
  {
   "cell_type": "code",
   "id": "nsg2iy1wcs",
   "metadata": {},
   "outputs": [],
   "source": "# Discover available models\nprint(\"Available providers:\")\nproviders = models.providers()\nfor provider in providers:\n    print(f\"  - {provider}\")\n\nprint(\"\\nAvailable models:\")\nall_models = models.list()\n\n# Group by provider\nfrom collections import defaultdict\nby_provider = defaultdict(list)\nfor model in all_models:\n    if model.startswith('gpt'):\n        by_provider['openai'].append(model)\n    elif model.startswith('claude'):\n        by_provider['anthropic'].append(model)\n    elif model.startswith('gemini'):\n        by_provider['google'].append(model)\n\nfor provider, models in sorted(by_provider.items()):\n    print(f\"\\n{provider.title()} ({len(models)} models):\")\n    for model in sorted(models)[:5]:  # Show first 5\n        print(f\"  - {model}\")\n    if len(models) > 5:\n        print(f\"  ... and {len(models) - 5} more\")"
  },
  {
   "cell_type": "code",
   "id": "models-basic",
   "metadata": {},
   "outputs": [],
   "source": "# Pattern 1: Direct invocation - perfect for one-off calls\nresponse = models(\"gpt-4o\", \"What is machine learning in one sentence?\")\nprint(\"Direct call response:\")\nprint(f\"  Text: {response.text}\")\nprint(f\"  Cost: ${response.usage['cost']:.4f}\")\nprint(f\"  Tokens: {response.usage['total_tokens']}\")"
  },
  {
   "cell_type": "code",
   "id": "models-binding",
   "metadata": {},
   "outputs": [],
   "source": "# Pattern 2: Model binding - reusable configurations\ncreative_writer = models.instance(\n    \"claude-3.5-sonnet-20241022\",\n    temperature=0.8,\n    system=\"You are a creative writer who uses vivid metaphors.\"\n)\n\ntechnical_writer = models.instance(\n    \"gpt-4o\",\n    temperature=0.2,\n    system=\"You are a technical writer who values precision and clarity.\"\n)\n\nprompt = \"Explain how a neural network works\"\n\nprint(\"Creative explanation:\")\nprint(creative_writer(prompt).text[:200] + \"...\\n\")\n\nprint(\"Technical explanation:\")\nprint(technical_writer(prompt).text[:200] + \"...\")"
  },
  {
   "cell_type": "code",
   "id": "models-cost-tracking",
   "metadata": {},
   "outputs": [],
   "source": "# Pattern 3: Cost tracking and usage monitoring\n# Make a few calls to accumulate usage\nfor i in range(3):\n    models(\"gpt-3.5-turbo\", f\"Generate a random fact #{i}\")\n\n# Get usage summary\nfrom ember.models.registry import get_global_registry\nregistry = get_global_registry()\nsummary = registry.get_usage_summary(\"gpt-3.5-turbo\")\n\nprint(\"Usage Summary:\")\nprint(f\"  Total calls: {summary.call_count}\")\nprint(f\"  Total tokens: {summary.total_tokens}\")\nprint(f\"  Total cost: ${summary.cost_usd:.4f}\")\nprint(f\"  Avg tokens/call: {summary.avg_tokens_per_call:.1f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Module 2: Data API - Streaming-First Processing\n",
    "\n",
    "Ember's data module is designed for memory-efficient processing of large datasets. Let's explore streaming patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream data without loading entire dataset into memory\n",
    "print(\"Streaming first 5 MMLU questions:\\n\")\n",
    "\n",
    "for i, item in enumerate(stream(\"mmlu\", subset=\"elementary_mathematics\")):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Q{i+1}: {item['question']}\")\n",
    "    print(f\"A: {item['answer']}\")\n",
    "    if 'choices' in item:\n",
    "        print(f\"Choices: {list(item['choices'].values())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Build a data processing pipeline\n",
    "def add_difficulty_score(item):\n",
    "    \"\"\"Add a difficulty score based on question length.\"\"\"\n",
    "    item[\"difficulty_score\"] = len(item[\"question\"]) / 100.0\n",
    "    return item\n",
    "\n",
    "# Chain operations for efficient processing\n",
    "hard_questions = (\n",
    "    stream(\"mmlu\", subset=\"elementary_mathematics\")\n",
    "    .transform(add_difficulty_score)\n",
    "    .filter(lambda x: x[\"difficulty_score\"] > 0.5)\n",
    "    .first(3)\n",
    ")\n",
    "\n",
    "print(\"Hard questions (based on length):\")\n",
    "for q in hard_questions:\n",
    "    print(f\"- {q['question'][:100]}...\")\n",
    "    print(f\"  Difficulty: {q['difficulty_score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operators-header",
   "metadata": {},
   "source": [
    "## Module 3: Operators - Composable AI Components\n",
    "\n",
    "Operators are the building blocks for complex AI systems. They can be simple functions or sophisticated components."
   ]
  },
  {
   "cell_type": "code",
   "id": "operators-simple",
   "metadata": {},
   "outputs": [],
   "source": "# Level 1: Simple function operator\n@op\ndef sentiment_analyzer(text: str) -> dict:\n    \"\"\"Analyze sentiment of text.\"\"\"\n    prompt = f\"Analyze the sentiment of this text. Return ONLY 'positive', 'negative', or 'neutral': {text}\"\n    sentiment = models(\"gpt-3.5-turbo\", prompt).text.strip().lower()\n    return {\"text\": text, \"sentiment\": sentiment}\n\n# Test it\nresult = sentiment_analyzer(\"I love this new framework! It makes AI development so much easier.\")\nprint(f\"Sentiment: {result['sentiment']}\")"
  },
  {
   "cell_type": "code",
   "id": "operators-class",
   "metadata": {},
   "outputs": [],
   "source": "# Level 2: Class-based operator with state\nfrom ember.operators import Operator\n\nclass Summarizer(Operator):\n    def __init__(self, style=\"concise\"):\n        self.style = style\n        self.model = models.instance(\n            \"gpt-3.5-turbo\",\n            system=f\"You are a {style} summarizer. Keep summaries brief.\"\n        )\n    \n    def forward(self, text: str) -> str:\n        prompt = f\"Summarize this text in a {self.style} way: {text}\"\n        return self.model(prompt).text\n\n# Create different summarizers\ntechnical_summary = Summarizer(\"technical\")\nsimple_summary = Summarizer(\"simple\")\n\ntext = \"\"\"Neural networks are computational models inspired by biological neural networks. \nThey consist of interconnected nodes (neurons) organized in layers. Each connection has \na weight that is adjusted during training through backpropagation.\"\"\"\n\nprint(\"Technical summary:\")\nprint(technical_summary(text))\nprint(\"\\nSimple summary:\")\nprint(simple_summary(text))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operators-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition patterns with different models\n",
    "from ember.operators import chain, ensemble\n",
    "\n",
    "# Chain operators sequentially - each optimized for its task\n",
    "@op\n",
    "def extract_key_points(text: str) -> list:\n",
    "    # GPT-4o for structured extraction\n",
    "    prompt = f\"Extract 3 key points from this text as a Python list: {text}\"\n",
    "    response = models(\"gpt-4o\", prompt).text\n",
    "    # Simple parsing - in production use structured output\n",
    "    return eval(response) if response.startswith('[') else [response]\n",
    "\n",
    "@op \n",
    "def expand_points(points: list) -> str:\n",
    "    # Claude for creative expansion\n",
    "    prompt = f\"Expand these points into a coherent, engaging paragraph: {points}\"\n",
    "    return models(\"claude-3-5-sonnet-20241022\", prompt).text\n",
    "\n",
    "@op\n",
    "def translate_to_spanish(text: str) -> str:\n",
    "    # Gemini for translation tasks\n",
    "    prompt = f\"Translate to Spanish: {text}\"\n",
    "    return models(\"gemini-pro\", prompt).text\n",
    "\n",
    "# Create a multi-model pipeline\n",
    "analysis_pipeline = chain(\n",
    "    extract_key_points,    # GPT-4o extracts structure\n",
    "    expand_points,         # Claude expands creatively\n",
    "    sentiment_analyzer     # Claude analyzes sentiment\n",
    ")\n",
    "\n",
    "# For translation pipeline\n",
    "translation_pipeline = chain(\n",
    "    extract_key_points,    # GPT-4o extracts key points\n",
    "    expand_points,         # Claude expands\n",
    "    translate_to_spanish   # Gemini translates\n",
    ")\n",
    "\n",
    "result = analysis_pipeline(text)\n",
    "print(\"Multi-model pipeline result:\")\n",
    "print(f\"Final sentiment of expanded summary: {result['sentiment']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show translation pipeline\n",
    "translation_result = translation_pipeline(text)\n",
    "print(\"Translation pipeline result (Spanish):\")\n",
    "print(translation_result[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble-header",
   "metadata": {},
   "source": [
    "## Module 4: Ensemble & Judge System for MMLU Evaluation\n",
    "\n",
    "Let's build a sophisticated evaluation system using ensembles of experts and judges."
   ]
  },
  {
   "cell_type": "code",
   "id": "ensemble-experts",
   "metadata": {},
   "outputs": [],
   "source": "# Create specialized experts for different subjects\nmath_expert = models.instance(\n    \"gpt-4o\",\n    system=\"You are a mathematics professor. Answer questions precisely with step-by-step reasoning.\"\n)\n\nscience_expert = models.instance(\n    \"claude-3-sonnet\", \n    system=\"You are a science teacher. Explain concepts clearly using examples.\"\n)\n\ngeneral_expert = models.instance(\n    \"gpt-3.5-turbo\",\n    system=\"You are a knowledgeable assistant. Provide accurate, well-reasoned answers.\"\n)\n\n# Function to format MMLU questions\ndef format_mmlu_question(item):\n    \"\"\"Format MMLU item for expert evaluation.\"\"\"\n    question = item[\"question\"]\n    if \"choices\" in item:\n        choices_text = \"\\n\".join([f\"{k}: {v}\" for k, v in item[\"choices\"].items()])\n        return f\"{question}\\n\\nChoices:\\n{choices_text}\\n\\nAnswer with the letter only.\"\n    return question"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-provider ensemble evaluation system\n",
    "def ensemble_evaluate(question_item, experts_dict):\n",
    "    \"\"\"Use multiple experts from different providers.\"\"\"\n",
    "    formatted_q = format_mmlu_question(question_item)\n",
    "    \n",
    "    # Get answers from all experts\n",
    "    expert_answers = []\n",
    "    for name, expert in experts_dict.items():\n",
    "        answer = expert(formatted_q).text.strip()\n",
    "        # Extract model info from the bound instance\n",
    "        model_id = expert._model if hasattr(expert, '_model') else name\n",
    "        expert_answers.append({\n",
    "            \"expert_name\": name,\n",
    "            \"model\": model_id,\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": 0.7 + (len(expert_answers) * 0.05)  # Simulated confidence\n",
    "        })\n",
    "    \n",
    "    return expert_answers\n",
    "\n",
    "# Create a judge using Claude (excellent at analysis and comparison)\n",
    "def judge_answers(question_item, expert_answers):\n",
    "    \"\"\"Use Claude as judge to select the best answer from multiple providers.\"\"\"\n",
    "    judge_prompt = f\"\"\"Given this question: {question_item['question']}\n",
    "\n",
    "Expert answers from different AI providers:\n",
    "{chr(10).join([f\"{a['expert_name']} ({a['model']}): {a['answer']}\" for a in expert_answers])}\n",
    "\n",
    "Select the best answer based on:\n",
    "1. Correctness and accuracy\n",
    "2. Clarity of reasoning\n",
    "3. Appropriateness to the question type\n",
    "\n",
    "Return ONLY the expert name (math_expert, science_expert, general_expert, or speed_expert).\"\"\"\n",
    "    \n",
    "    # Claude is excellent at following complex instructions\n",
    "    judge = models.instance(\"claude-3-haiku\", temperature=0.1)\n",
    "    judge_response = judge(judge_prompt).text.strip()\n",
    "    \n",
    "    # Find the selected answer\n",
    "    for ans in expert_answers:\n",
    "        if ans[\"expert_name\"] in judge_response:\n",
    "            return ans[\"answer\"], ans[\"expert_name\"], ans[\"model\"]\n",
    "    \n",
    "    # Fallback to highest confidence\n",
    "    best = max(expert_answers, key=lambda x: x[\"confidence\"])\n",
    "    return best[\"answer\"], best[\"expert_name\"], best[\"model\"]\n",
    "\n",
    "# Test on a sample MMLU question\n",
    "sample_question = next(iter(stream(\"mmlu\", subset=\"elementary_mathematics\")))\n",
    "print(f\"Question: {sample_question['question']}\")\n",
    "print(f\"Correct answer: {sample_question['answer']}\\n\")\n",
    "\n",
    "# Run multi-provider ensemble evaluation\n",
    "experts = {\n",
    "    \"math_expert\": math_expert,\n",
    "    \"science_expert\": science_expert,\n",
    "    \"general_expert\": general_expert,\n",
    "    \"speed_expert\": speed_expert\n",
    "}\n",
    "\n",
    "expert_answers = ensemble_evaluate(sample_question, experts)\n",
    "\n",
    "print(\"Expert answers from different providers:\")\n",
    "for ans in expert_answers:\n",
    "    print(f\"  {ans['expert_name']} ({ans['model']}): {ans['answer']}\")\n",
    "\n",
    "# Judge selects best answer\n",
    "final_answer, selected_expert, selected_model = judge_answers(sample_question, expert_answers)\n",
    "print(f\"\\nJudge (Claude) selected: {selected_expert} ({selected_model})\")\n",
    "print(f\"Final answer: {final_answer}\")\n",
    "print(f\"Correct: {final_answer == sample_question['answer']}\")\n",
    "\n",
    "print(\"\\nThis demonstrates true multi-provider ensemble:\")\n",
    "print(\"- OpenAI (GPT-4o) for mathematical reasoning\")\n",
    "print(\"- Anthropic (Claude 3.5) for scientific explanation\") \n",
    "print(\"- Google (Gemini) for general knowledge\")\n",
    "print(\"- OpenAI (GPT-4o-mini) for quick responses\")\n",
    "print(\"- Anthropic (Claude Haiku) as judge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcs-header",
   "metadata": {},
   "source": [
    "## Module 5: XCS - Zero-Config Optimization\n",
    "\n",
    "XCS automatically optimizes your Python functions by detecting parallelization opportunities. Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xcs-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate slow operations that can be parallelized\n",
    "def slow_operation(x, delay=0.1):\n",
    "    \"\"\"Simulate a slow operation.\"\"\"\n",
    "    time.sleep(delay)\n",
    "    return x * 2\n",
    "\n",
    "# Without optimization\n",
    "def process_sequential(items):\n",
    "    results = []\n",
    "    for item in items:\n",
    "        # These operations are independent\n",
    "        r1 = slow_operation(item, 0.05)\n",
    "        r2 = slow_operation(item * 2, 0.05)\n",
    "        r3 = slow_operation(item * 3, 0.05)\n",
    "        results.append(r1 + r2 + r3)\n",
    "    return results\n",
    "\n",
    "# With XCS optimization\n",
    "@jit\n",
    "def process_optimized(items):\n",
    "    results = []\n",
    "    for item in items:\n",
    "        # XCS detects these can run in parallel!\n",
    "        r1 = slow_operation(item, 0.05)\n",
    "        r2 = slow_operation(item * 2, 0.05) \n",
    "        r3 = slow_operation(item * 3, 0.05)\n",
    "        results.append(r1 + r2 + r3)\n",
    "    return results\n",
    "\n",
    "# Compare performance\n",
    "test_items = [1, 2, 3, 4, 5]\n",
    "\n",
    "print(\"Sequential execution:\")\n",
    "start = time.time()\n",
    "result_seq = process_sequential(test_items)\n",
    "seq_time = time.time() - start\n",
    "print(f\"Time: {seq_time:.2f}s\")\n",
    "\n",
    "print(\"\\nOptimized execution (first run includes tracing):\")\n",
    "start = time.time()\n",
    "result_opt = process_optimized(test_items)\n",
    "opt_time_first = time.time() - start\n",
    "print(f\"Time: {opt_time_first:.2f}s\")\n",
    "\n",
    "print(\"\\nOptimized execution (subsequent runs):\")\n",
    "start = time.time()\n",
    "result_opt = process_optimized(test_items)\n",
    "opt_time = time.time() - start\n",
    "print(f\"Time: {opt_time:.2f}s\")\n",
    "print(f\"\\nSpeedup: {seq_time/opt_time:.1f}x\")\n",
    "\n",
    "# Show optimization details\n",
    "if hasattr(process_optimized, '_xcs_explain'):\n",
    "    print(\"\\nOptimization explanation:\")\n",
    "    print(process_optimized._xcs_explain())"
   ]
  },
  {
   "cell_type": "code",
   "id": "xcs-real-world",
   "metadata": {},
   "outputs": [],
   "source": "# Real-world example: Parallel LLM calls\n@jit\ndef analyze_text_parallel(text):\n    \"\"\"Analyze text from multiple perspectives in parallel.\"\"\"\n    # These LLM calls are independent and will run in parallel\n    sentiment = models(\"gpt-3.5-turbo\", f\"Sentiment of: {text}\").text\n    summary = models(\"gpt-3.5-turbo\", f\"Summarize: {text}\").text  \n    keywords = models(\"gpt-3.5-turbo\", f\"Extract keywords from: {text}\").text\n    \n    return {\n        \"sentiment\": sentiment,\n        \"summary\": summary,\n        \"keywords\": keywords\n    }\n\n# Test with real text\nsample_text = \"\"\"Ember is a revolutionary AI framework that makes building \nproduction AI systems simple and efficient. It provides clean APIs, \nautomatic optimization, and seamless multi-provider support.\"\"\"\n\nprint(\"Analyzing text with parallel LLM calls...\")\nstart = time.time()\nanalysis = analyze_text_parallel(sample_text)\nprint(f\"\\nCompleted in {time.time() - start:.2f}s\")\nprint(f\"\\nResults:\")\nfor key, value in analysis.items():\n    print(f\"{key}: {value[:100]}...\" if len(value) > 100 else f\"{key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "id": "jax-header",
   "metadata": {},
   "source": [
    "## Module 6: Advanced - JAX Integration with Mixed Structures\n",
    "\n",
    "Ember's deep JAX integration enables gradient-based optimization of AI systems with mixed static/dynamic components."
   ]
  },
  {
   "cell_type": "code",
   "id": "jax-mixed-structure",
   "metadata": {},
   "outputs": [],
   "source": "# Create a learnable router with mixed structure\nfrom ember.operators import Operator\n\nclass LearnableRouter(Operator):\n    \"\"\"Routes prompts to different models based on learned embeddings.\"\"\"\n    \n    def __init__(self, routes: dict, embed_dim: int = 32, key: jax.Array = None):\n        # Static fields (not learnable)\n        self.routes = routes  # Dict of model names to model instances\n        self.route_names = list(routes.keys())\n        \n        # Dynamic fields (learnable via gradients)\n        if key is None:\n            key = jax.random.PRNGKey(42)\n        k1, k2 = jax.random.split(key)\n        \n        self.route_embeddings = jax.random.normal(k1, (len(routes), embed_dim))\n        self.projection = jax.random.normal(k2, (embed_dim, 1)) \n        self.temperature = jnp.array(1.0)\n    \n    def compute_routing_scores(self, prompt: str) -> jnp.ndarray:\n        \"\"\"Compute routing scores for each model.\"\"\"\n        # Simple: use prompt length as feature (in practice, use embeddings)\n        prompt_feature = jnp.array([len(prompt) / 100.0] * self.route_embeddings.shape[1])\n        \n        # Compute similarity scores\n        similarities = self.route_embeddings @ prompt_feature\n        scores = jax.nn.softmax(similarities / self.temperature)\n        return scores\n    \n    def forward(self, prompt: str) -> dict:\n        \"\"\"Route prompt to best model and return response.\"\"\"\n        scores = self.compute_routing_scores(prompt)\n        best_idx = jnp.argmax(scores)\n        \n        # Use the selected model\n        selected_route = self.route_names[best_idx]\n        response = self.routes[selected_route](prompt)\n        \n        return {\n            \"response\": response.text,\n            \"selected_model\": selected_route,\n            \"scores\": scores,\n            \"confidence\": float(jnp.max(scores))\n        }\n\n# Create router with different models\nsimple_model = models.instance(\"gpt-3.5-turbo\", temperature=0.3)\ncreative_model = models.instance(\"claude-3-sonnet\", temperature=0.9) \n\nrouter = LearnableRouter({\n    \"simple\": simple_model,\n    \"creative\": creative_model\n})\n\n# Test routing\ntest_prompts = [\n    \"What is 2+2?\",\n    \"Write a poem about the ocean\"\n]\n\nfor prompt in test_prompts:\n    result = router(prompt)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Selected: {result['selected_model']} (confidence: {result['confidence']:.2f})\")\n    print(f\"Response: {result['response'][:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jax-gradient-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the router using gradients\n",
    "def routing_loss(router, prompt: str, target_route: str) -> float:\n",
    "    \"\"\"Compute loss for routing decision.\"\"\"\n",
    "    scores = router.compute_routing_scores(prompt)\n",
    "    target_idx = router.route_names.index(target_route)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    return -jnp.log(scores[target_idx] + 1e-8)\n",
    "\n",
    "# Training data: short prompts -> simple, long prompts -> creative\n",
    "training_data = [\n",
    "    (\"What is 2+2?\", \"simple\"),\n",
    "    (\"Calculate 10*5\", \"simple\"),\n",
    "    (\"Define AI\", \"simple\"),\n",
    "    (\"Write a story about a magical forest where time flows backwards\", \"creative\"),\n",
    "    (\"Compose a sonnet about the beauty of mathematics\", \"creative\"),\n",
    "    (\"Create a metaphor for machine learning using cooking\", \"creative\")\n",
    "]\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "opt_state = optimizer.init(router)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training router...\")\n",
    "for epoch in range(50):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for prompt, target in training_data:\n",
    "        # Compute loss and gradients\n",
    "        loss_val = routing_loss(router, prompt, target)\n",
    "        grads = jax.grad(routing_loss)(router, prompt, target)\n",
    "        \n",
    "        # Update only dynamic parameters\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        \n",
    "        # Apply updates using functional update pattern\n",
    "        router = router.update_params(\n",
    "            route_embeddings=router.route_embeddings + updates.route_embeddings,\n",
    "            projection=router.projection + updates.projection,\n",
    "            temperature=router.temperature + updates.temperature\n",
    "        )\n",
    "        \n",
    "        total_loss += float(loss_val)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTesting trained router:\")\n",
    "test_cases = [\n",
    "    \"What is 5+5?\",\n",
    "    \"Imagine a world where colors have sounds\"\n",
    "]\n",
    "\n",
    "for prompt in test_cases:\n",
    "    result = router(prompt)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Routed to: {result['selected_model']} (confidence: {result['confidence']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jax-advanced-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient flow through mixed structures\n",
    "print(\"Analyzing gradient flow through mixed structure:\\n\")\n",
    "\n",
    "# Get gradients for a sample\n",
    "sample_prompt = \"Write a haiku\"\n",
    "grads = jax.grad(routing_loss)(router, sample_prompt, \"creative\")\n",
    "\n",
    "# Check which fields received gradients\n",
    "print(\"Fields with gradients (dynamic/learnable):\")\n",
    "for field in ['route_embeddings', 'projection', 'temperature']:\n",
    "    if hasattr(grads, field) and getattr(grads, field) is not None:\n",
    "        grad_val = getattr(grads, field)\n",
    "        print(f\"  - {field}: shape {grad_val.shape}, norm {jnp.linalg.norm(grad_val):.4f}\")\n",
    "\n",
    "print(\"\\nStatic fields (preserved but no gradients):\")\n",
    "for field in ['routes', 'route_names']:\n",
    "    if hasattr(grads, field):\n",
    "        print(f\"  - {field}: {type(getattr(grads, field)).__name__}\")\n",
    "\n",
    "# Demonstrate JAX transformations work seamlessly\n",
    "print(\"\\nJAX transformations on mixed structures:\")\n",
    "\n",
    "# Vectorize routing decisions\n",
    "batch_prompts = [\"Hello\", \"What is AI?\", \"Tell me a story\"]\n",
    "vmapped_scores = jax.vmap(router.compute_routing_scores)(jnp.array(batch_prompts))\n",
    "print(f\"Batched routing scores shape: {vmapped_scores.shape}\")\n",
    "\n",
    "print(\"\\n✅ Mixed structure gradients working correctly!\")\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"- JAX arrays are automatically treated as learnable parameters\")\n",
    "print(\"- Python objects (dicts, strings) are preserved as static\")\n",
    "print(\"- Gradients flow only through dynamic fields\")\n",
    "print(\"- Standard optimizers (optax) work out of the box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Let's combine everything we've learned into a complete system for MMLU evaluation with learnable routing."
   ]
  },
  {
   "cell_type": "code",
   "id": "complete-system",
   "metadata": {},
   "outputs": [],
   "source": "# Complete MMLU evaluation system with all components\n@jit\ndef evaluate_mmlu_batch(questions, router, judge_model):\n    \"\"\"Evaluate a batch of MMLU questions using ensemble + judge + routing.\"\"\"\n    results = []\n    \n    for q in questions:\n        # Step 1: Route to appropriate expert ensemble\n        routing_result = router(q['question'])\n        \n        # Step 2: Get multiple expert opinions (parallelized by XCS)\n        expert1_answer = models(\"gpt-3.5-turbo\", format_mmlu_question(q)).text\n        expert2_answer = models(\"gpt-3.5-turbo\", format_mmlu_question(q)).text\n        expert3_answer = models(\"gpt-3.5-turbo\", format_mmlu_question(q)).text\n        \n        # Step 3: Judge selects best answer\n        judge_prompt = f\"\"\"Select the best answer:\n        Expert 1: {expert1_answer}\n        Expert 2: {expert2_answer}  \n        Expert 3: {expert3_answer}\n        \n        Return only the expert number (1, 2, or 3).\"\"\"\n        \n        judge_decision = judge_model(judge_prompt).text.strip()\n        \n        # Step 4: Record results\n        selected_answer = [expert1_answer, expert2_answer, expert3_answer][int(judge_decision)-1]\n        results.append({\n            'question': q['question'],\n            'predicted': selected_answer,\n            'correct': q['answer'],\n            'routing_confidence': routing_result['confidence']\n        })\n    \n    return results\n\n# Load a small batch of MMLU questions\nmmlu_batch = list(stream(\"mmlu\", subset=\"elementary_mathematics\").first(3))\n\n# Create judge model\njudge = models.instance(\"gpt-3.5-turbo\", temperature=0.1)\n\n# Run evaluation\nprint(\"Running complete MMLU evaluation pipeline...\\n\")\nresults = evaluate_mmlu_batch(mmlu_batch, router, judge)\n\n# Calculate accuracy\ncorrect = sum(1 for r in results if r['predicted'].strip() == r['correct'].strip())\naccuracy = correct / len(results)\n\nprint(f\"Results:\")\nfor r in results:\n    print(f\"Q: {r['question'][:50]}...\")\n    print(f\"   Predicted: {r['predicted']}, Correct: {r['correct']}\")\n    print(f\"   Routing confidence: {r['routing_confidence']:.2f}\\n\")\n\nprint(f\"Accuracy: {accuracy:.1%} ({correct}/{len(results)})\")\n\n# Show cost summary\ntotal_cost = sum(s.cost_usd for s in [registry.get_usage_summary(m) \n                                      for m in [\"gpt-3.5-turbo\"]])\nprint(f\"\\nTotal cost for this session: ${total_cost:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've learned the core concepts of Ember:\n",
    "\n",
    "1. **Models API** - Unified interface for all LLM providers with automatic cost tracking\n",
    "2. **Data API** - Memory-efficient streaming for large datasets\n",
    "3. **Operators** - Composable building blocks from simple functions to complex systems\n",
    "4. **Ensemble & Judge** - Sophisticated evaluation patterns for accuracy\n",
    "5. **XCS** - Automatic optimization without configuration\n",
    "6. **JAX Integration** - Gradients on mixed structures for learnable AI systems\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Explore more examples** in the `examples/` directory\n",
    "2. **Read the documentation** for detailed API references\n",
    "3. **Build your own operators** for your specific use cases\n",
    "4. **Experiment with different models** and providers\n",
    "5. **Join the community** to share ideas and get help\n",
    "\n",
    "### Key Principles to Remember:\n",
    "\n",
    "- **Simple things should be simple** - One-line model calls\n",
    "- **Complex things should be possible** - Full JAX integration\n",
    "- **Zero configuration** - Works out of the box\n",
    "- **Performance by default** - Automatic optimization\n",
    "- **Clean architecture** - Principled design throughout\n",
    "\n",
    "Happy building with Ember!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}